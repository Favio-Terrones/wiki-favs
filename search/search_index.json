{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenido a la Wiki DataLake","text":"<p>\u00a1Te damos la bienvenida a nuestro espacio central de conocimiento! Aqu\u00ed encontrar\u00e1s toda la informaci\u00f3n necesaria sobre procesos, proyectos, lineamientos y mejores pr\u00e1cticas que utilizamos en el Equipo de Big Data. Esta wiki est\u00e1 dise\u00f1ada para ser el punto de referencia principal, de manera que puedas acceder f\u00e1cilmente a la documentaci\u00f3n y colaborar con el crecimiento de la organizaci\u00f3n.</p>"},{"location":"#vision","title":"Visi\u00f3n","text":"<p>Ser el motor de la transformaci\u00f3n Data Driven en la empresa, facilitando la toma de decisiones estrat\u00e9gicas y la creaci\u00f3n de valor a trav\u00e9s del an\u00e1lisis y la gesti\u00f3n inteligente de datos.</p>"},{"location":"#mision","title":"Misi\u00f3n","text":"<p>Impulsar la captura de valor mediante la autogesti\u00f3n en el uso de los datos, democratizando la informaci\u00f3n de forma confiable y oportuna, para que el uso de los datos sea un activo fundamental para el negocio.</p>"},{"location":"#estructura-organizacional","title":"Estructura Organizacional","text":"<p>A continuaci\u00f3n, se muestra el organigrama que describe la estructura de nuestro Equipo de Big Data y sus frentes de trabajo principales.</p> <p></p>"},{"location":"#roles-principales","title":"Roles Principales","text":"<ul> <li> <p>Gerente de Big Data   Responsable de la direcci\u00f3n estrat\u00e9gica y coordinaci\u00f3n de todos los frentes de trabajo.</p> </li> <li> <p>Frente QA   Encargado de asegurar la calidad de los datos y la validaci\u00f3n de los entregables anal\u00edticos.</p> </li> <li> <p>Frente Cloud   Especializado en la arquitectura en la nube, la infraestructura y la seguridad de los entornos Big Data.</p> </li> <li> <p>Frente DataOps   Encargado de la automatizaci\u00f3n y optimizaci\u00f3n de los flujos de datos, promoviendo la colaboraci\u00f3n entre equipos de desarrollo y operaciones para garantizar la confiabilidad, escalabilidad y agilidad para la entrega de nuevos componentes.</p> </li> <li> <p>Frente BI y Modelamiento de Datos   Dedicado a la construcci\u00f3n de modelos anal\u00edticos y dashboards que faciliten la toma de decisiones.</p> </li> <li> <p>Frente Comercial &amp; Supply   Focalizado en el relacionamiento con clientes internos y externos, as\u00ed como la gesti\u00f3n de la cadena de suministro de datos.</p> </li> <li> <p>Frente Finanzas &amp; Programas   Responsable de la planificaci\u00f3n financiera de los proyectos y la gesti\u00f3n de programas de transformaci\u00f3n digital.</p> </li> <li> <p>Frente Self Management Team (SMT)   Orientado al desarrollo de la cultura de autogesti\u00f3n y la optimizaci\u00f3n de procesos internos.</p> </li> </ul> <p>Nota: Este organigrama puede variar seg\u00fan la evoluci\u00f3n de los proyectos y la estructura interna del equipo.</p>"},{"location":"#secciones-principales","title":"Secciones Principales","text":"<p>Para facilitar la navegaci\u00f3n, hemos organizado la wiki en varias secciones. Cada secci\u00f3n contiene gu\u00edas, procedimientos, est\u00e1ndares y mejores pr\u00e1cticas relevantes.</p> <ol> <li>Proyectos </li> <li>Descripci\u00f3n de los principales proyectos de Big Data en curso.  </li> <li>Roadmaps, objetivos y estados de avance.  </li> <li> <p>Metodolog\u00edas de seguimiento y reporting.</p> </li> <li> <p>Procesos y Procedimientos </p> </li> <li>Flujos de trabajo para la ingesta de datos, ETL/ELT y almacenamiento.  </li> <li>Lineamientos de aseguramiento de calidad y pruebas.  </li> <li> <p>Pol\u00edticas de seguridad y gobernanza de datos.</p> </li> <li> <p>Herramientas y Tecnolog\u00edas </p> </li> <li>Descripci\u00f3n de la stack tecnol\u00f3gica (Cloud, herramientas de BI, pipelines, etc.).  </li> <li>Gu\u00edas de instalaci\u00f3n, configuraci\u00f3n y mantenimiento.  </li> <li> <p>Recomendaciones de mejores pr\u00e1cticas.</p> </li> <li> <p>Gu\u00edas y Tutoriales </p> </li> <li>Tutoriales paso a paso para uso de plataformas internas.  </li> <li>Casos de uso y ejemplos pr\u00e1cticos.  </li> <li> <p>Documentaci\u00f3n de APIs y librer\u00edas internas.</p> </li> <li> <p>Recursos y Referencias </p> </li> <li>Documentos de referencia, libros blancos y art\u00edculos recomendados.  </li> <li>Enlaces a repositorios internos o externos.  </li> <li> <p>Est\u00e1ndares y normativas a nivel corporativo o legal.</p> </li> <li> <p>FAQ (Preguntas Frecuentes) </p> </li> <li>Respuestas a las dudas m\u00e1s comunes sobre los procesos y herramientas.  </li> <li>Consejos r\u00e1pidos para la resoluci\u00f3n de problemas habituales.</li> </ol>"},{"location":"#contribuir-a-la-wiki","title":"Contribuir a la Wiki","text":"<p>Nuestra wiki est\u00e1 en constante evoluci\u00f3n. Para mantenerla actualizada y confiable:</p> <ol> <li>Proponer Cambios: Si detectas informaci\u00f3n desactualizada o deseas agregar contenido, crea un Pull Request (o el flujo que tu organizaci\u00f3n utilice) en el repositorio correspondiente.  </li> <li>Revisi\u00f3n: Todo cambio es revisado por el equipo de QA y/o el responsable del frente correspondiente.  </li> <li>Aprobaci\u00f3n: Una vez validado, se fusiona el contenido al repositorio principal y se despliega en la wiki.</li> </ol> <p>Sugerencia: Aseg\u00farate de incluir referencias y ejemplos claros para facilitar la comprensi\u00f3n y adopci\u00f3n de tus aportes.</p>"},{"location":"#creditos-y-agradecimientos","title":"Cr\u00e9ditos y Agradecimientos","text":"<p>Agradecemos a todos los miembros del equipo que contribuyen diariamente a la construcci\u00f3n y mejora de esta wiki. Tu conocimiento y dedicaci\u00f3n hacen posible que el Equipo de Big Data siga creciendo y generando valor en la organizaci\u00f3n.</p>"},{"location":"#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<ul> <li>Revisa las Secciones Principales para ubicar la informaci\u00f3n que necesites.  </li> <li>Si eres nuevo en el equipo, consulta la Gu\u00eda de Onboarding (enlace de ejemplo) para acelerar tu proceso de adaptaci\u00f3n.  </li> <li>Participa en las discusiones y contribuye con tus conocimientos para que esta wiki sea cada vez m\u00e1s completa.</li> </ul> <p>\u00a1Bienvenido a la WikiLake y gracias por ser parte de esta gran aventura en Big Data!</p>"},{"location":"dataops/index-dataops/","title":"Sobre Nosotros","text":""},{"location":"dataops/index-dataops/#objetivo","title":"Objetivo","text":"<p>Responsables de automatizar y agilizar los despliegues de datos, a trav\u00e9s de herramientas de integraci\u00f3n continua y despliegue continuo. A su  vez, somos responsables de desarrollar componentes que fomenten la automatizaci\u00f3n de procesos, creaci\u00f3n de est\u00e1ndares y mejora de  interacci\u00f3n entre equipos</p>"},{"location":"dataops/index-dataops/#nuestro-equipo","title":"Nuestro Equipo","text":""},{"location":"dataops/index-dataops/#integrantes","title":"Integrantes","text":""},{"location":"dataops/index-dataops/#1-jose-taboada","title":"1. Jose Taboada","text":"<p> Rol: Lider de Frente  </p>"},{"location":"dataops/index-dataops/#2-favio-terrones","title":"2. Favio Terrones","text":"<p> Rol: Ingeniero de Datos  </p>"},{"location":"dataops/index-dataops/#actividades-del-equipo","title":"Actividades del Equipo","text":"<p>En esta secci\u00f3n se detallan las iniciativas y mejoras en las que estamos trabajando para optimizar nuestros procesos y herramientas.</p>"},{"location":"dataops/index-dataops/#automatizacion-y-cicd","title":"Automatizaci\u00f3n y CI/CD","text":"<ul> <li>Automatizaci\u00f3n y mejoras de herramientas CI/CD</li> <li>Estandarizaci\u00f3n de flujo de procesos y creaci\u00f3n de plantillas</li> </ul>"},{"location":"dataops/index-dataops/#big-data-y-desarrollo","title":"Big Data y Desarrollo","text":"<ul> <li>Adopci\u00f3n de metodolog\u00eda \u00e1gil</li> <li>Desarrollo de nuevos componentes que reduzcan el lead time de desarrollo</li> <li>Creaci\u00f3n de componentes CROSS para responder a consultas manuales y eliminar la carga operativa</li> <li>Monitoreo del estado de salud del Datalake (Pr\u00f3ximamente)</li> </ul> <p>Nota: Si tienen alguna sugerencia o mejora en automatizaci\u00f3n/eficiencia, por favor contacten a sus LT's o l\u00edderes de frente.</p>"},{"location":"dataops/bigquery/calculadora-costos/","title":"Calculadora de Costos","text":""},{"location":"dataops/bigquery/calculadora-costos/#proposito-general","title":"Prop\u00f3sito General","text":"<p>El objetivo principal de este proceso es integrar y cruzar informaci\u00f3n proveniente de distintas vistas de metadata que nos proporciona BigQuery para obtener un an\u00e1lisis detallado de la creaci\u00f3n de tablas y su relaci\u00f3n con los costos asociados. En concreto, se busca:</p> <ul> <li> <p>Asociar Metadata y Costos:   Unir datos de ejecuci\u00f3n de jobs (con sus respectivos costos y m\u00e9tricas) con la metadata de los DAGs de Airflow que generan estas tablas.</p> </li> <li> <p>Distribuci\u00f3n del Costo:   Determinar la cantidad de veces que una tabla de origen es utilizada en la creaci\u00f3n de diferentes tablas (concurrencia) y prorratear el costo total asociado en funci\u00f3n de dicho uso.</p> </li> <li> <p>Optimizaci\u00f3n y Transparencia:   Proveer una visi\u00f3n integral que facilite la evaluaci\u00f3n del uso de recursos, permitiendo optimizar y ajustar los procesos de transformaci\u00f3n y creaci\u00f3n de tablas en el entorno de BigQuery.</p> </li> </ul>"},{"location":"dataops/bigquery/calculadora-costos/#bigquery","title":"BigQuery","text":"<p>El proceso se estructura en varios componentes clave dentro de BigQuery:</p>"},{"location":"dataops/bigquery/calculadora-costos/#1-extraccion-de-datos","title":"1. Extracci\u00f3n de Datos","text":"<ul> <li>Se utilizan las vistas de:</li> <li><code>INFORMATION_SCHEMA.JOBS_BY_ORGANIZATION</code></li> <li><code>INFORMATION_SCHEMA.RESERVATION_CHANGES_BY_PROJECT</code></li> </ul> <p>para capturar datos sobre los jobs ejecutados y los cambios en reservas, respectivamente.</p>"},{"location":"dataops/bigquery/calculadora-costos/#2-transformacion-mediante-ctes","title":"2. Transformaci\u00f3n mediante CTEs","text":"<p>Se definen CTEs (Common Table Expressions) para modularizar la l\u00f3gica:</p> <ul> <li> <p>reservation_slot:   Captura los cambios en reservas en un rango de fechas.</p> </li> <li> <p>jobs:   Extrae y calcula m\u00e9tricas b\u00e1sicas de los jobs (como bytes facturados, duraci\u00f3n y slots utilizados).</p> </li> <li> <p>maestro_costos:   Calcula los costos asociados a cada job, diferenciando entre modos de facturaci\u00f3n.</p> </li> <li> <p>DAGS_MAESTRO:   Extrae la metadata de los DAGs, identificando:</p> </li> <li><code>dag_id</code></li> <li>La tabla creada</li> <li>Las tablas origen involucradas.</li> </ul>"},{"location":"dataops/bigquery/calculadora-costos/#3-integracion-y-analisis","title":"3. Integraci\u00f3n y An\u00e1lisis","text":"<ul> <li> <p>Uni\u00f3n de Datos:   Se realiza la uni\u00f3n de la informaci\u00f3n de costos con la metadata de los DAGs mediante una llave de cruce (tabla creada / <code>table_path_id</code>).</p> </li> <li> <p>C\u00e1lculo de Concurrencia y Prorrateo:   Se aplican funciones de ventana para:</p> </li> <li>Calcular la concurrencia de uso de cada tabla de origen.</li> <li>Calcular el costo prorrateado (dividiendo el costo total entre el n\u00famero de usos).</li> </ul>"},{"location":"dataops/bigquery/calculadora-costos/#proceso-bronze","title":"Proceso Bronze","text":"<p>Este proceso nos permite capturar los costos asociados a las tablas de bronze-prod por d\u00eda.  </p> <p></p>"},{"location":"dataops/bigquery/calculadora-costos/#proceso-silver-golden-delivery","title":"Proceso Silver-Golden-Delivery","text":"<p>Este proceso nos permite capturar los costos asociados a las tablas silver-golden-delivery por d\u00eda.</p> <p></p>"},{"location":"dataops/bigquery/calculadora-costos/#proceso-generacion-calculadora","title":"Proceso Generaci\u00f3n Calculadora","text":"<p>Este proceso nos permita aplicar una l\u00f3gica recursiva para obtener la trazabilidad en t\u00e9rminos monetarios de una tabla final  con las tablas que participaron en su creaci\u00f3n.  Se tiene el consolidado por tabla , asi como el detallado.</p> <p></p>"},{"location":"dataops/bigquery/calculadora-costos/#diagrama-de-secuencia","title":"Diagrama de Secuencia","text":""},{"location":"dataops/bigquery/calculadora-costos/#codigo-fuente-nueva-capa-lod","title":"C\u00f3digo Fuente (Nueva capa Lod)","text":"<ul> <li>Capa Lake Operation Data</li> </ul>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/","title":"Dag - Execution","text":""},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#indice","title":"\u00cdndice","text":"<ol> <li>Diagrama de Funcionamiento</li> <li>Diagrama de Registro de Nuevo DAG</li> <li>Archivo Parameters y Tablas de Ingesta - Dependencia</li> <li>Archivo Maestro de DAGs</li> </ol>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#diagrama-de-funcionamiento","title":"Diagrama de Funcionamiento","text":"<ol> <li> <p>El proceso inicia con la ejecuci\u00f3n autom\u00e1tica del dag trigger_01. Para ello, se apoya en la informaci\u00f3n de las tablas <code>config_tablas_ingesta</code>, <code>config_dependencias_dag</code> y en el archivo <code>parameters.json</code>.</p> </li> <li> <p>Una vez terminado el trigger_01 (l\u00f3gica de dependencias de dag), se ejecuta el dag trigger_02. Este dag realiza la ejecuci\u00f3n en orden, seg\u00fan las dependencias de dag y por ambiente (silver, golden y delivery).</p> </li> </ol>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#diagrama-de-registro-de-nuevo-dag","title":"Diagrama de Registro de Nuevo DAG","text":"<ol> <li> <p>Se debe registrar la dependencia entre dags en el archivo <code>parameters.json</code> en el modelo nuevo.</p> </li> <li> <p>Luego se deben registrar las tablas que se crearon, ya sea en el ambiente silver, golden y delivery, y las tablas que se usen del ambiente bronze en la tabla <code>config_tablas_ingesta</code>. En la tabla <code>config_dependencias_dag</code> se registra el nuevo dag y las tablas del ambiente anterior que se necesitan para que el dag se ejecute.</p> </li> <li> <p>Finalmente, se registra el nombre del nuevo dag en el archivo <code>slv_gld_dags_to_trigger</code>.</p> </li> </ol>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#archivo-parameters-y-tablas-de-ingesta-dependencia","title":"Archivo Parameters y Tablas de Ingesta - Dependencia","text":"<p>En este archivo JSON se registran las dependencias entre dags. Solo se registra el nombre del dag, ya sea del mismo ambiente o del ambiente anterior, en la parte de <code>dag_dependencies</code>. Se toma como ejemplo el dag de gld cliente.</p>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#ejemplo-de-registro-en-el-archivo-parameters","title":"Ejemplo de Registro en el Archivo Parameters","text":"<p>El dag del ambiente golden, para que se ejecute correctamente y con la data actualizada, depende de 3 dags del ambiente anterior que es silver:</p> <ul> <li><code>alicorp-pe-gtm-digitales-{env}-slv-modelo-cliente</code></li> <li><code>alicorp-pe-gtm-digitales-{env}-slv-modelo-producto</code></li> <li><code>alicorp-pe-gtm-digitales-{env}-slv-modelo-ventas</code></li> </ul> <p>Podr\u00eda ser que tambi\u00e9n dependa de otro dag del ambiente golden. En ese caso, se debe registrar tambi\u00e9n el nombre del dag en la parte de <code>dag_dependencies</code>.</p>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#donde-se-encuentra-el-archivo","title":"\u00bfD\u00f3nde se encuentra el Archivo?","text":"<ul> <li>El archivo se encuentra en la carpeta <code>dml</code> del modelo que se est\u00e1 desarrollando.  </li> <li>Debe ser creado tomando como referencia el archivo de otro modelo.  </li> <li>Todo se trabaja a trav\u00e9s de GitHub, donde se pueden encontrar los proyectos de Bronze, Silver y Golden y guiarse de los modelos existentes.</li> </ul>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#tabla-de-ingesta","title":"Tabla de Ingesta","text":"<p>Esta tabla contiene todos los nombres de las tablas que se usan en los dags, ya sean de bronze, silver, golden y delivery. Funciona como una tabla de monitoreo, ya que permite conocer qu\u00e9 tablas se usan en el Datalake.</p>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#funcionamiento-segun-ambiente","title":"Funcionamiento seg\u00fan Ambiente","text":"<ol> <li>Si se ejecuta un dag de silver, la tabla contiene las tablas de bronze y silver que usa el dag.</li> <li>Si se ejecuta un dag de golden, contiene tablas de silver y golden.</li> <li>Si se ejecuta un dag de delivery, contiene tablas de golden y delivery.</li> </ol>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#estructura-de-la-tabla-de-ingesta","title":"Estructura de la Tabla de Ingesta","text":"Esquema Descripci\u00f3n <ul><li>Nombre del dataset</li><li>Nombre de la tabla</li><li>Identificador de la tabla</li><li>Estado de actualizaci\u00f3n de la tabla</li><li>Estado de la tabla despu\u00e9s del proceso de actualizaci\u00f3n</li><li>Indica si la cantidad increment\u00f3</li><li>Campo usado como flag</li><li>Fecha de actualizaci\u00f3n de la tabla</li><li>Cantidad de registros</li><li>Variaci\u00f3n de cantidad</li><li>Indica si la tabla se carg\u00f3 correctamente</li><li>Plataforma a la que pertenece la tabla</li></ul> <p>nota: No es necesario completar todas las columnas si no se tiene el detalle de cada una de ellas. El insert general que se utiliza es el siguiente:</p>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#ejemplo-de-insert","title":"Ejemplo de Insert","text":"<pre><code>INSERT INTO `acpe-prod-brz.PROD_CONFIG.config_tablas_ingesta` \nVALUES('PROD__SAPS4','t001k','',true,true,false,false,CURRENT_TIMESTAMP(),0,0,true,'SAPS4');\n</code></pre>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#campos-indispensables","title":"Campos indispensables","text":"<ul> <li>Dataset</li> <li>Table_name</li> <li>Status_last_ejecution </li> <li>Si el valor es <code>true</code>, no es necesario validar la fecha de la tabla en el campo <code>last_ejecution</code>.</li> <li>Si es <code>false</code>, se debe validar que la fecha en <code>last_ejecution</code> est\u00e9 actualizada.</li> <li>Este campo es esencial para la ejecuci\u00f3n del DAG en silver; para golden y delivery puede ir en <code>true</code>.</li> <li>Last_ejecution   Indica la fecha de actualizaci\u00f3n de la tabla, con valor por defecto <code>CURRENT_TIMESTAMP()</code>.</li> <li>Plataforma</li> </ul> <p>Los otros campos puede ir con valores por defecto como se indica en el insert siguiente:</p> <pre><code>INSERT INTO `acpe-prod-brz.PROD_CONFIG.config_tablas_ingesta` \nVALUES(\u2018{dataset}\u2019,\u2019{table_name}','',true,\u2019{status_last_ejecution}',false,false,\u2019{last_ejecution}',0,0,true,'{plataforma}');\n</code></pre>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#tabla-de-dependencia","title":"Tabla de Dependencia","text":"<p>Esta tabla contiene los nombres de los dags que se ejecutan en el dag execution y cada dag contiene las tablas que se utilizan en la l\u00f3gica de los dml(.sql), estas tablas deben estar registradas en la tabla config_tablas_ingesta para que el proceso de dag execution ejecute correctamente.</p> Esquema Descripci\u00f3n <ul><li>Nombre del equipo al que pertenece el dag</li><li>Descripci\u00f3n general del dag</li><li>Descripci\u00f3n detallada del dag</li><li>Nombre del dag</li><li>Frecuencia de actualizaci\u00f3n (d\u00eda, mes, a\u00f1o)</li><li>Fecha de inicio de ejecuci\u00f3n del dag</li><li>Indica si el dag ejecut\u00f3 correctamente</li><li>Fecha fin de ejecuci\u00f3n del dag</li><li>Nombre del dataset de la tabla dependiente</li><li>Nombre de la tabla dependiente</li><li>Ambiente de la tabla dependiente</li><li>Grado de dependencia con la tabla dependiente</li><li>D\u00edas de tolerancia para actualizaci\u00f3n de la tabla dependiente</li></ul> <p>No es necesario completar todas las columnas si no se tiene el detalle de cada una de ellas. El insert general que se utiliza es el siguiente:</p> <pre><code>INSERT INTO `acpe-prod-brz.PROD_CONFIG.config_dependencias_dag` \nVALUES\n('SUPPLY','MODELO MAESTRO','tablas del modelo maestro','alicorp-pe-s4-dd-slv-modelo-maestro','DIARIA',CURRENT_TIMESTAMP(),'SUCCEEDED',CURRENT_TIMESTAMP(),'PROD__SAPS4','t001l','BRONZE','DIARIA',0);\n</code></pre>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#campos-indispensables_1","title":"Campos indispensables","text":"<ul> <li>equipo</li> <li>nombre_descriptivo_dag</li> <li>descripci\u00f3n </li> <li>recurso </li> <li>dataset</li> <li>nombre_tabla</li> <li>tipo_ingesta</li> </ul> <p>Los otros campos puede ir con valores por defecto como se indica en el insert siguiente:</p> <pre><code>INSERT INTO `acpe-prod-brz.PROD_CONFIG.config_dependencias_dag` \nVALUES(\u2018{equipo}\u2019,\u2019{nombre_descriptivo_dag}\u2019,\u2019{descripcion}\u2019,\u2019{recurso}','DIARIA',CURRENT_TIMESTAMP(),'SUCCEEDED',CURRENT_TIMESTAMP(),\u2019{dataset}\u2019,\u2019{nombre_tabla}\u2019,\u2019{tipo_ingesta}','DIARIA',0);\n</code></pre>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#archivo-maestro-de-dags","title":"Archivo Maestro de Dags","text":"<p>En este archivo json se registra le nombre del dag para que aparezca en el proceso de ejecuci\u00f3n del dag trigger_02, de lo contrario no va a aparecer el nombre.</p> <p> </p>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/","title":"Plantillas Cl\u00e1sicas","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#proposito-general","title":"Prop\u00f3sito General","text":"<p>Orquestar la ejecuci\u00f3n de scripts SQL en BigQuery para procesar datos relacionados con un modelo de caso de uso Alicorp. Los datos fluyen a trav\u00e9s de 3 capas:</p> <ul> <li>SLV (Silver): Transformaciones iniciales y limpieza.</li> <li>GLD (Gold): Enriquecimiento y agregaciones.</li> <li>DLV (Delivery): Datos finales para consumo.</li> </ul>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#tipos","title":"Tipos","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#carga-completa","title":"Carga Completa","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#descripcion","title":"Descripci\u00f3n","text":"<p>Definici\u00f3n: Plantilla utilizada para realizar la carga total de la entidad sin aplicar filtros de tiempo, garantizando una copia \u00edntegra de la capa anterior.</p> <p>Objetivo: - Asegurar la integridad de la informaci\u00f3n.  - Empleada en la capa Silver y en los maestros de la capa Golden.</p>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#funcionamiento","title":"Funcionamiento","text":"<p>Proceso:   Se ejecuta un script SQL donde se reemplazan las variables definidas en <code>parameters</code> para efectuar una carga total.</p> <p>Caracter\u00edsticas:   - Cada task generado es \u00fanico.    - Garantiza la ejecuci\u00f3n \u00edntegra del script configurado.</p>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#grafo","title":"Grafo","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#carga-mixta","title":"Carga Mixta","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#descripcion_1","title":"Descripci\u00f3n","text":"<p>Definici\u00f3n: Plantilla que integra la carga completa y la carga por partici\u00f3n, ideal para DAGs que incluyen tanto maestros como datos transaccionales.</p> <p>Objetivo: - Prevenir la duplicaci\u00f3n de la carga de maestros.  - Mantener la integridad y sincronizaci\u00f3n de la informaci\u00f3n en entornos h\u00edbridos.</p>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#funcionamiento_1","title":"Funcionamiento","text":"<p>Proceso:   Se ejecutan inicialmente los tasks basados en particiones y, posteriormente, se procesan los tasks correspondientes a los maestros.</p> <p>Caracter\u00edsticas:   - Combina los m\u00e9todos de Carga Completa y Carga por Partici\u00f3n.    - Configuraci\u00f3n que asegura la correcta ejecuci\u00f3n sin conflictos entre procesos.</p>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#grafo_1","title":"Grafo","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#carga-por-particion","title":"Carga Por Partici\u00f3n","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#descripcion_2","title":"Descripci\u00f3n","text":"<p>Definici\u00f3n: Plantilla orientada a la carga de tablas particionadas por periodo, permitiendo actualizar solo la informaci\u00f3n necesaria mediante filtros de fecha.</p> <p>Objetivo: - Actualizar \u00fanicamente los datos requeridos.  - Evitar la transferencia de informaci\u00f3n hist\u00f3rica, lo que reduce costos operativos.</p>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#funcionamiento_2","title":"Funcionamiento","text":"<p>Proceso:   Se crea un segundo task que se ejecuta hasta la fecha l\u00edmite (d\u00eda de corte).</p> <p>Caracter\u00edsticas:   - Genera una cadena adicional basada en el mes anterior para actualizar la porci\u00f3n de datos correspondiente.    - Emplea un bucle para la generaci\u00f3n de tasks.    - Se identifica mediante el bloque que contiene el campo <code>\"month\"</code>.</p>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#grafo_2","title":"Grafo","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#carga-completa_1","title":"Carga Completa","text":"<p>Plantilla .py: Descargar plantilla</p> <p>Parameters .json:  Descargar paremeters.json</p>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#carga-mixta_1","title":"Carga Mixta","text":"<p>Plantilla .py: Descargar plantilla</p> <p>Parameters .json:  Descargar paremeters.json</p>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/clasicos/#carga-por-particion_1","title":"Carga Por Partici\u00f3n","text":"<p>Plantilla .py: Descargar plantilla</p> <p>Parameters .json:  Descargar paremeters.json</p>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/","title":"Plantillas Custom","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#transversal-actualizacion-con-condicionales","title":"TRANSVERSAL - ACTUALIZACI\u00d3N CON CONDICIONALES","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#proposito-general","title":"Prop\u00f3sito General","text":"<p>Orquestar la ejecuci\u00f3n de scripts SQL en BigQuery para procesar datos relacionados con un modelo de caso de uso Alicorp. Los datos fluyen a trav\u00e9s de 3 capas:</p> <ul> <li>SLV (Silver): Transformaciones iniciales y limpieza.</li> <li>GLD (Gold): Enriquecimiento y agregaciones.</li> <li>DLV (Delivery): Datos finales para consumo.</li> </ul>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#caracteristicas-clave","title":"Caracter\u00edsticas Clave","text":"<ul> <li> <p>Programaci\u00f3n:   Ejecuci\u00f3n cada n horas (0 /n * * ).</p> </li> <li> <p>Manejo de Errores: </p> </li> <li>Notificaciones por email en caso de fallos.  </li> <li> <p>Reintentos autom\u00e1ticos (1 reintento cada 2 minutos).</p> </li> <li> <p>Configuraci\u00f3n Din\u00e1mica:   Generaci\u00f3n autom\u00e1tica de tareas basadas en <code>MAPPING_CARPETAS</code>.</p> </li> <li> <p>L\u00f3gica Horaria:   Ejecuta tareas hist\u00f3ricas solo entre 00:00 y 0n:00 (hora Lima).</p> </li> </ul>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#estructura-del-flujo","title":"Estructura del Flujo","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#1-componentes-principales","title":"1. Componentes Principales","text":"Elemento Tipo Descripci\u00f3n <code>all_failed</code> <code>ErrorHandlerOperator</code> Actualiza estado a FAILED en BigQuery si se detectan fallos. <code>all_success</code> <code>BigQueryInsertJobOperator</code> Actualiza estado a SUCCEEDED en BigQuery si todas las tareas se ejecutan con \u00e9xito. <code>validacion_horario</code> <code>BranchPythonOperator</code> Determina si se debe ejecutar una tarea hist\u00f3rica o marcar el \u00e9xito seg\u00fan la hora actual. Tareas SQL <code>BigQueryInsertJobOperator</code> 24 tareas generadas din\u00e1micamente a partir de <code>MAPPING_CARPETAS</code>."},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#2-capas-de-datos","title":"2. Capas de Datos","text":"Capa Descripci\u00f3n Ejemplos de Tablas SLV Datos crudos transformados <code>s4_documento_entrega_cabecera</code>, <code>tms_orden_flete_base</code> GLD Datos enriquecidos <code>s4_avance_control_facturacion_letra</code>, <code>tms_avance_control_facturacion</code> DLV Datos listos para consumo <code>cr_avance_control_facturacion_historico</code>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#3-dependencias-clave","title":"3. Dependencias Clave","text":"<ul> <li>SLV_s4_documento_entrega_cabecera \u2192 SLV_s4_documento_envio_cabecera \u2192 GLD_tms_avance_control_facturacion</li> <li>SLV_tms_orden_flete_gestion \u2192 GLD_tms_avance_control_facturacion \u2192 GLD_s4_avance_control_facturacion_letra \u2192 DLV_cr_avance_control_facturacion_actualizado</li> </ul>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#configuracion-tecnica","title":"Configuraci\u00f3n T\u00e9cnica","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#variables-de-entorno","title":"Variables de Entorno","text":"Variable Valor Funci\u00f3n <code>EMAIL_RECIPIENTS</code> <code>['ext_mdelgadoa@alicorp.com.pe', ...]</code> Destinatarios de alertas. <code>periodo_anterior</code> Fecha del mes anterior (formato <code>YYYY-MM-DD</code>) Reemplazo en scripts SQL."},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#seguridad","title":"Seguridad","text":"<ul> <li>Impersonaci\u00f3n de Cuentas de Servicio:   Cada tarea utiliza <code>impersonation_chain</code> para ejecutar el flujo con una GSA custom que posee permisos espec\u00edficos.</li> </ul>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#flujo-de-ejecucion","title":"Flujo de Ejecuci\u00f3n","text":"<ol> <li> <p>Inicio:    Todas las tareas SQL se ejecutan en el orden definido por las dependencias.</p> </li> <li> <p>Validaci\u00f3n Horaria: </p> </li> <li>Entre 00:00 y 0n:00 (hora Lima): Ejecuta <code>DLV_cr_avance_control_facturacion_historico</code>.  </li> <li> <p>Fuera de este horario: Se salta a la actualizaci\u00f3n del estado a SUCCEEDED.</p> </li> <li> <p>Finalizaci\u00f3n: </p> </li> <li>Exitoso: <code>all_success</code> actualiza el estado en BigQuery.  </li> <li>Fallido: <code>all_failed</code> registra el error y notifica.</li> </ol>"},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#grafo-de-ejecucion","title":"Grafo de Ejecuci\u00f3n","text":""},{"location":"dataops/cloudComposer/DagExecution/plantillas/customs/#plantilla","title":"Plantilla","text":"<p>C\u00f3digo Fuente:  Descargar plantilla</p>"},{"location":"dataops/cloudFunction/cloudFunctionParquet/","title":"Carga archivos .parquet a BigQuery","text":""},{"location":"dataops/cloudFunction/cloudFunctionParquet/#proposito-general","title":"Prop\u00f3sito General","text":"<p>Orquestar la carga autom\u00e1tica hacia BigQuery desde archivos <code>.parquet</code> depositados en un bucket de Cloud Storage.</p> <ul> <li>Schema Detection: <code>us-{env}-com-gcf-trv-schema-detection</code></li> <li>Carga BQ Unigis: <code>us-{env}-com-gcf-bq-process-load</code></li> </ul>"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#consideraciones","title":"Consideraciones","text":"<ul> <li>La extensi\u00f3n \u00fanica aceptada es <code>.parquet</code></li> <li>El nombre de la cabecera en los archivos debe considerar lo siguiente:</li> <li>Solo contener letras, n\u00fameros y guiones bajos</li> <li>Comenzar con una letra o gui\u00f3n bajo</li> <li>Tener un m\u00e1ximo de 300 caracteres</li> <li>Todo campo/columna ser\u00e1 cargado a BigQuery con tipo de dato <code>STRING</code>, a pesar de que en el archivo de carga tenga un tipo de dato diferente</li> <li>Por defecto, cada vez que se cargue un nuevo <code>.parquet</code> se a\u00f1adir\u00e1 como append a la tabla, con el nuevo contenido del archivo, pero con el mismo schema</li> </ul>"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#arquitectura-base-ejemplo-unigis-process","title":"Arquitectura Base - Ejemplo Unigis Process","text":""},{"location":"dataops/cloudFunction/cloudFunctionParquet/#recursos","title":"Recursos","text":""},{"location":"dataops/cloudFunction/cloudFunctionParquet/#desarrollo-dev","title":"Desarrollo (dev)","text":"Servicio Nombre Proyecto Descripci\u00f3n Cloud Storage <code>us_dev_stg_gst_&lt;iniciativa&gt;</code> <code>acpe-dev-brz-tmp</code> Bucket que almacena los archivos de la iniciativa asociada Cloud Functions <code>us-dev-com-gcf-trv-schema-detection</code> <code>acpe-dev-brz-tmp</code> Funci\u00f3n que detecta el schema del archivo cargado Cloud Functions <code>us-dev-com-gcf-bq-process-load</code> <code>acpe-dev-brz-tmp</code> Funci\u00f3n que se encarga de cargar la data hacia BigQuery en la capa BRONZE BigQuery - <code>acpe-dev-brz</code> Proyecto en el cual se carga la informaci\u00f3n procesada"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#calidad-qa","title":"Calidad (qa)","text":"Servicio Nombre Proyecto Descripci\u00f3n Cloud Storage <code>us_qa_stg_gst_&lt;iniciativa&gt;</code> <code>acpe-qa-brz-tmp</code> Bucket que almacena los archivos de la iniciativa asociada Cloud Functions <code>us-qa-com-gcf-trv-schema-detection</code> <code>acpe-qa-brz-tmp</code> Funci\u00f3n que detecta el schema del archivo cargado Cloud Functions <code>us-qa-com-gcf-bq-process-load</code> <code>acpe-qa-brz-tmp</code> Funci\u00f3n que se encarga de cargar la data hacia BigQuery en la capa BRONZE BigQuery - <code>acpe-qa-brz</code> Proyecto en el cual se carga la informaci\u00f3n procesada"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#produccion-prod","title":"Producci\u00f3n (prod)","text":"Servicio Nombre Proyecto Descripci\u00f3n Cloud Storage <code>us_prod_stg_gst_&lt;iniciativa&gt;</code> <code>acpe-prod-brz-tmp</code> Bucket que almacena los archivos de la iniciativa asociada Cloud Functions <code>us-prod-com-gcf-trv-schema-detection</code> <code>acpe-prod-brz-tmp</code> Funci\u00f3n que detecta el schema del archivo cargado Cloud Functions <code>us-prod-com-gcf-bq-process-load</code> <code>acpe-prod-brz-tmp</code> Funci\u00f3n que se encarga de cargar la data hacia BigQuery en la capa BRONZE BigQuery - <code>acpe-prod-brz</code> Proyecto en el cual se carga la informaci\u00f3n procesada"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#codigo-fuente","title":"C\u00f3digo Fuente","text":"<ul> <li>Schema Detection</li> <li>Carga BQ Unigis</li> </ul>"},{"location":"dataops/utilidades/bash/bash-001/","title":"GitHub Workflow Bash","text":""},{"location":"dataops/utilidades/bash/bash-001/#objetivo","title":"Objetivo","text":"<p>Este script Bash automatiza la validaci\u00f3n y actualizaci\u00f3n de workflows en repositorios de GitHub pertenecientes a una organizaci\u00f3n o usuario espec\u00edfico. Su funci\u00f3n es garantizar que los archivos de workflow usen inicialmente <code>ubuntu-20.04</code> para luego actualizarlos a <code>ubuntu-24.04</code> de forma autom\u00e1tica, integrando los cambios en la rama especificada.</p>"},{"location":"dataops/utilidades/bash/bash-001/#funcionalidades","title":"Funcionalidades","text":"<ul> <li>Automatizaci\u00f3n: Clona repositorios y procesa cada uno sin intervenci\u00f3n manual.</li> <li>Validaci\u00f3n: Verifica que todos los workflows tengan configurado <code>runs-on: ubuntu-20.04</code> antes de actualizar.</li> <li>Actualizaci\u00f3n: Reemplaza la versi\u00f3n de Ubuntu en los archivos YAML y realiza commit y push a la rama especificada.</li> <li>Limpieza: Elimina los repositorios clonados una vez finalizado el proceso.</li> </ul>"},{"location":"dataops/utilidades/bash/bash-001/#codigo-del-script","title":"C\u00f3digo del Script","text":"<pre><code>#!/bin/bash  \n# Variables: configuramos nuestro usuario (o nombre de la organizaci\u00f3n) y, si es necesario, token\nUSERNAME=\"Alicorp-Digital\"\n# Opcional: exportamos el token si GitHub CLI lo requiere, por ejemplo:\n# export GITHUB_TOKEN=\"tu_token\"\n\n# Listamos todos los repositorios\nrepos=$(gh repo list $USERNAME --limit 1000 --json nameWithOwner -q '.[].nameWithOwner')\n\n\nfor repo in $repos; do\n  echo \"$repo...\"\n  git clone https://github.com/$repo.git\n  repo_dir=$(basename $repo)\n  cd \"$repo_dir\" || continue\n\n  # Verificamos que exista el directorio de workflows\n  if [ -d \".github/workflows\" ]; then\n\n    # Validamos que TODOS los workflows tengan en sus l\u00edneas 'runs-on:' solo \"ubuntu-20.04\"\n    valid=true\n    for file in .github/workflows/*.yml; do\n      if [ -f \"$file\" ]; then\n        # Extraemos las l\u00edneas que contengan \"runs-on:\"\n        runs_on_lines=$(grep \"runs-on:\" \"$file\")\n        if [ -n \"$runs_on_lines\" ]; then\n          # Si alguna de las l\u00edneas no contiene \"ubuntu-20.04\", se marca el repo como no v\u00e1lido\n          if echo \"$runs_on_lines\" | grep -q -v \"ubuntu-20.04\"; then\n             valid=false\n             echo \"Skipping $repo: $file tiene una l\u00ednea 'runs-on:' que no es ubuntu-20.04\"\n             break\n          fi\n        fi\n      fi\n    done\n\n    if [ \"$valid\" = false ]; then\n      echo \"Skipping repository $repo because not all workflows use exclusively ubuntu-20.04\"\n      cd ..\n      rm -rf \"$repo_dir\"\n      continue\n    fi\n\n    # Actualizamos cada archivo .yml dentro de la carpeta workflows\n    for file in .github/workflows/*.yml; do\n      if [ -f \"$file\" ]; then\n        echo \"Actualizando $file...\"\n        # Reemplazamos versiones antiguas por ubuntu-24.04\n        sed -i 's/runs-on: ubuntu-[0-9]\\+\\(\\.[0-9]\\+\\)\\?/runs-on: ubuntu-24.04/g' \"$file\"\n      fi\n    done\n\n    # Si hubo cambios, los comitea y empuja\n    if [ -n \"$(git status --porcelain)\" ]; then\n      git add .github/workflows\n      git commit -m \"[no ci] Actualizaci\u00f3n autom\u00e1tica: workflow a ubuntu-24.04\"\n      # Pusheamos a la rama que necesitamos\n      git push origin prod\n    fi\n  else\n    echo \"No se encontr\u00f3 el directorio .github/workflows en $repo\"\n  fi\n\n  cd ..\n  # Limpiamos el repositorio clonado\n  rm -rf \"$repo_dir\"\ndone\n\n</code></pre>"}]}