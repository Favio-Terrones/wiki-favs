{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenido a la WikiLake V3","text":"<p>\u00a1Te damos la bienvenida a nuestro espacio central de conocimiento! Aqu\u00ed encontrar\u00e1s toda la informaci\u00f3n necesaria sobre procesos, proyectos, lineamientos y mejores pr\u00e1cticas que utilizamos en el Equipo de Big Data. Esta wiki est\u00e1 dise\u00f1ada para ser el punto de referencia principal, de manera que puedas acceder f\u00e1cilmente a la documentaci\u00f3n y colaborar con el crecimiento de la organizaci\u00f3n.</p>"},{"location":"#vision","title":"Visi\u00f3n","text":"<p>Ser el motor de la transformaci\u00f3n Data Driven en la empresa, facilitando la toma de decisiones estrat\u00e9gicas y la creaci\u00f3n de valor a trav\u00e9s del an\u00e1lisis y la gesti\u00f3n inteligente de datos.</p>"},{"location":"#mision","title":"Misi\u00f3n","text":"<p>Impulsar la captura de valor mediante la autogesti\u00f3n en el uso de los datos, democratizando la informaci\u00f3n de forma confiable y oportuna, para que el uso de los datos sea un activo fundamental para el negocio.</p>"},{"location":"#estructura-organizacional","title":"Estructura Organizacional","text":"<p>A continuaci\u00f3n, se muestra el organigrama que describe la estructura de nuestro Equipo de Big Data y sus frentes de trabajo principales.</p> <p></p>"},{"location":"#roles-principales","title":"Roles Principales","text":"<ul> <li> <p>Gerente de Big Data   Responsable de la direcci\u00f3n estrat\u00e9gica y coordinaci\u00f3n de todos los frentes de trabajo.</p> </li> <li> <p>Frente QA   Encargado de asegurar la calidad de los datos y la validaci\u00f3n de los entregables anal\u00edticos.</p> </li> <li> <p>Frente Cloud   Especializado en la arquitectura en la nube, la infraestructura y la seguridad de los entornos Big Data.</p> </li> <li> <p>Frente DataOps   Encargado de la automatizaci\u00f3n y optimizaci\u00f3n de los flujos de datos, promoviendo la colaboraci\u00f3n entre equipos de desarrollo y operaciones para garantizar la confiabilidad, escalabilidad y agilidad para la entrega de nuevos componentes.</p> </li> <li> <p>Frente BI y Modelamiento de Datos   Dedicado a la construcci\u00f3n de modelos anal\u00edticos y dashboards que faciliten la toma de decisiones.</p> </li> <li> <p>Frente Comercial &amp; Supply   Focalizado en el relacionamiento con clientes internos y externos, as\u00ed como la gesti\u00f3n de la cadena de suministro de datos.</p> </li> <li> <p>Frente Finanzas &amp; Programas   Responsable de la planificaci\u00f3n financiera de los proyectos y la gesti\u00f3n de programas de transformaci\u00f3n digital.</p> </li> <li> <p>Frente Self Management Team (SMT)   Orientado al desarrollo de la cultura de autogesti\u00f3n y la optimizaci\u00f3n de procesos internos.</p> </li> </ul> <p>Nota: Este organigrama puede variar seg\u00fan la evoluci\u00f3n de los proyectos y la estructura interna del equipo.</p>"},{"location":"#secciones-principales","title":"Secciones Principales","text":"<p>Para facilitar la navegaci\u00f3n, hemos organizado la wiki en varias secciones. Cada secci\u00f3n contiene gu\u00edas, procedimientos, est\u00e1ndares y mejores pr\u00e1cticas relevantes.</p> <ol> <li>Proyectos </li> <li>Descripci\u00f3n de los principales proyectos de Big Data en curso.  </li> <li>Roadmaps, objetivos y estados de avance.  </li> <li> <p>Metodolog\u00edas de seguimiento y reporting.</p> </li> <li> <p>Procesos y Procedimientos </p> </li> <li>Flujos de trabajo para la ingesta de datos, ETL/ELT y almacenamiento.  </li> <li>Lineamientos de aseguramiento de calidad y pruebas.  </li> <li> <p>Pol\u00edticas de seguridad y gobernanza de datos.</p> </li> <li> <p>Herramientas y Tecnolog\u00edas </p> </li> <li>Descripci\u00f3n de la stack tecnol\u00f3gica (Cloud, herramientas de BI, pipelines, etc.).  </li> <li>Gu\u00edas de instalaci\u00f3n, configuraci\u00f3n y mantenimiento.  </li> <li> <p>Recomendaciones de mejores pr\u00e1cticas.</p> </li> <li> <p>Gu\u00edas y Tutoriales </p> </li> <li>Tutoriales paso a paso para uso de plataformas internas.  </li> <li>Casos de uso y ejemplos pr\u00e1cticos.  </li> <li> <p>Documentaci\u00f3n de APIs y librer\u00edas internas.</p> </li> <li> <p>Recursos y Referencias </p> </li> <li>Documentos de referencia, libros blancos y art\u00edculos recomendados.  </li> <li>Enlaces a repositorios internos o externos.  </li> <li> <p>Est\u00e1ndares y normativas a nivel corporativo o legal.</p> </li> <li> <p>FAQ (Preguntas Frecuentes) </p> </li> <li>Respuestas a las dudas m\u00e1s comunes sobre los procesos y herramientas.  </li> <li>Consejos r\u00e1pidos para la resoluci\u00f3n de problemas habituales.</li> </ol>"},{"location":"#contribuir-a-la-wiki","title":"Contribuir a la Wiki","text":"<p>Nuestra wiki est\u00e1 en constante evoluci\u00f3n. Para mantenerla actualizada y confiable:</p> <ol> <li>Proponer Cambios: Si detectas informaci\u00f3n desactualizada o deseas agregar contenido, crea un Pull Request (o el flujo que tu organizaci\u00f3n utilice) en el repositorio correspondiente.  </li> <li>Revisi\u00f3n: Todo cambio es revisado por el equipo de QA y/o el responsable del frente correspondiente.  </li> <li>Aprobaci\u00f3n: Una vez validado, se fusiona el contenido al repositorio principal y se despliega en la wiki.</li> </ol> <p>Sugerencia: Aseg\u00farate de incluir referencias y ejemplos claros para facilitar la comprensi\u00f3n y adopci\u00f3n de tus aportes.</p>"},{"location":"#creditos-y-agradecimientos","title":"Cr\u00e9ditos y Agradecimientos","text":"<p>Agradecemos a todos los miembros del equipo que contribuyen diariamente a la construcci\u00f3n y mejora de esta wiki. Tu conocimiento y dedicaci\u00f3n hacen posible que el Equipo de Big Data siga creciendo y generando valor en la organizaci\u00f3n.</p>"},{"location":"#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<ul> <li>Revisa las Secciones Principales para ubicar la informaci\u00f3n que necesites.  </li> <li>Si eres nuevo en el equipo, consulta la Gu\u00eda de Onboarding (enlace de ejemplo) para acelerar tu proceso de adaptaci\u00f3n.  </li> <li>Participa en las discusiones y contribuye con tus conocimientos para que esta wiki sea cada vez m\u00e1s completa.</li> </ul> <p>\u00a1Bienvenido a la WikiLake y gracias por ser parte de esta gran aventura en Big Data!</p>"},{"location":"dataops/cloudFunction/cloudFunctionParquet/","title":"CloudFunction - Carga archivos .parquet a BigQuery","text":""},{"location":"dataops/cloudFunction/cloudFunctionParquet/#proposito-general","title":"Prop\u00f3sito General","text":"<p>Orquestar la carga autom\u00e1tica hacia BigQuery desde archivos <code>.parquet</code> depositados en un bucket de Cloud Storage.</p> <ul> <li>Schema Detection: <code>us-{env}-com-gcf-trv-schema-detection</code></li> <li>Carga BQ Unigis: <code>us-{env}-com-gcf-bq-process-load</code></li> </ul>"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#consideraciones","title":"Consideraciones","text":"<ul> <li>La extensi\u00f3n \u00fanica aceptada es <code>.parquet</code></li> <li>El nombre de la cabecera en los archivos debe considerar lo siguiente:</li> <li>Solo contener letras, n\u00fameros y guiones bajos</li> <li>Comenzar con una letra o gui\u00f3n bajo</li> <li>Tener un m\u00e1ximo de 300 caracteres</li> <li>Todo campo/columna ser\u00e1 cargado a BigQuery con tipo de dato <code>STRING</code>, a pesar de que en el archivo de carga tenga un tipo de dato diferente</li> <li>Por defecto, cada vez que se cargue un nuevo <code>.parquet</code> se a\u00f1adir\u00e1 como append a la tabla, con el nuevo contenido del archivo, pero con el mismo schema</li> </ul>"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#arquitectura-base-ejemplo-unigis-process","title":"Arquitectura Base - Ejemplo Unigis Process","text":""},{"location":"dataops/cloudFunction/cloudFunctionParquet/#recursos","title":"Recursos","text":""},{"location":"dataops/cloudFunction/cloudFunctionParquet/#desarrollo-dev","title":"Desarrollo (dev)","text":"Servicio Nombre Proyecto Descripci\u00f3n Cloud Storage <code>us_dev_stg_gst_&lt;iniciativa&gt;</code> <code>acpe-dev-brz-tmp</code> Bucket que almacena los archivos de la iniciativa asociada Cloud Functions <code>us-dev-com-gcf-trv-schema-detection</code> <code>acpe-dev-brz-tmp</code> Funci\u00f3n que detecta el schema del archivo cargado Cloud Functions <code>us-dev-com-gcf-bq-process-load</code> <code>acpe-dev-brz-tmp</code> Funci\u00f3n que se encarga de cargar la data hacia BigQuery en la capa BRONZE BigQuery - <code>acpe-dev-brz</code> Proyecto en el cual se carga la informaci\u00f3n procesada"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#calidad-qa","title":"Calidad (qa)","text":"Servicio Nombre Proyecto Descripci\u00f3n Cloud Storage <code>us_qa_stg_gst_&lt;iniciativa&gt;</code> <code>acpe-qa-brz-tmp</code> Bucket que almacena los archivos de la iniciativa asociada Cloud Functions <code>us-qa-com-gcf-trv-schema-detection</code> <code>acpe-qa-brz-tmp</code> Funci\u00f3n que detecta el schema del archivo cargado Cloud Functions <code>us-qa-com-gcf-bq-process-load</code> <code>acpe-qa-brz-tmp</code> Funci\u00f3n que se encarga de cargar la data hacia BigQuery en la capa BRONZE BigQuery - <code>acpe-qa-brz</code> Proyecto en el cual se carga la informaci\u00f3n procesada"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#produccion-prod","title":"Producci\u00f3n (prod)","text":"Servicio Nombre Proyecto Descripci\u00f3n Cloud Storage <code>us_prod_stg_gst_&lt;iniciativa&gt;</code> <code>acpe-prod-brz-tmp</code> Bucket que almacena los archivos de la iniciativa asociada Cloud Functions <code>us-prod-com-gcf-trv-schema-detection</code> <code>acpe-prod-brz-tmp</code> Funci\u00f3n que detecta el schema del archivo cargado Cloud Functions <code>us-prod-com-gcf-bq-process-load</code> <code>acpe-prod-brz-tmp</code> Funci\u00f3n que se encarga de cargar la data hacia BigQuery en la capa BRONZE BigQuery - <code>acpe-prod-brz</code> Proyecto en el cual se carga la informaci\u00f3n procesada"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#codigo-fuente","title":"C\u00f3digo Fuente","text":"<ul> <li>Schema Detection</li> <li>Carga BQ Unigis</li> </ul>"}]}