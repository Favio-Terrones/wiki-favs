{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenido a la Wiki DataLake","text":"<p>\u00a1Te damos la bienvenida a nuestro espacio central de conocimiento! Aqu\u00ed encontrar\u00e1s toda la informaci\u00f3n necesaria sobre procesos, proyectos, lineamientos y mejores pr\u00e1cticas que utilizamos en el Equipo de Big Data. Esta wiki est\u00e1 dise\u00f1ada para ser el punto de referencia principal, de manera que puedas acceder f\u00e1cilmente a la documentaci\u00f3n y colaborar con el crecimiento de la organizaci\u00f3n.</p>"},{"location":"#vision","title":"Visi\u00f3n","text":"<p>Ser el motor de la transformaci\u00f3n Data Driven en la empresa, facilitando la toma de decisiones estrat\u00e9gicas y la creaci\u00f3n de valor a trav\u00e9s del an\u00e1lisis y la gesti\u00f3n inteligente de datos.</p>"},{"location":"#mision","title":"Misi\u00f3n","text":"<p>Impulsar la captura de valor mediante la autogesti\u00f3n en el uso de los datos, democratizando la informaci\u00f3n de forma confiable y oportuna, para que el uso de los datos sea un activo fundamental para el negocio.</p>"},{"location":"#estructura-organizacional","title":"Estructura Organizacional","text":"<p>A continuaci\u00f3n, se muestra el organigrama que describe la estructura de nuestro Equipo de Big Data y sus frentes de trabajo principales.</p> <p></p>"},{"location":"#roles-principales","title":"Roles Principales","text":"<ul> <li> <p>Gerente de Big Data   Responsable de la direcci\u00f3n estrat\u00e9gica y coordinaci\u00f3n de todos los frentes de trabajo.</p> </li> <li> <p>Frente QA   Encargado de asegurar la calidad de los datos y la validaci\u00f3n de los entregables anal\u00edticos.</p> </li> <li> <p>Frente Cloud   Especializado en la arquitectura en la nube, la infraestructura y la seguridad de los entornos Big Data.</p> </li> <li> <p>Frente DataOps   Encargado de la automatizaci\u00f3n y optimizaci\u00f3n de los flujos de datos, promoviendo la colaboraci\u00f3n entre equipos de desarrollo y operaciones para garantizar la confiabilidad, escalabilidad y agilidad para la entrega de nuevos componentes.</p> </li> <li> <p>Frente BI y Modelamiento de Datos   Dedicado a la construcci\u00f3n de modelos anal\u00edticos y dashboards que faciliten la toma de decisiones.</p> </li> <li> <p>Frente Comercial &amp; Supply   Focalizado en el relacionamiento con clientes internos y externos, as\u00ed como la gesti\u00f3n de la cadena de suministro de datos.</p> </li> <li> <p>Frente Finanzas &amp; Programas   Responsable de la planificaci\u00f3n financiera de los proyectos y la gesti\u00f3n de programas de transformaci\u00f3n digital.</p> </li> <li> <p>Frente Self Management Team (SMT)   Orientado al desarrollo de la cultura de autogesti\u00f3n y la optimizaci\u00f3n de procesos internos.</p> </li> </ul> <p>Nota: Este organigrama puede variar seg\u00fan la evoluci\u00f3n de los proyectos y la estructura interna del equipo.</p>"},{"location":"#secciones-principales","title":"Secciones Principales","text":"<p>Para facilitar la navegaci\u00f3n, hemos organizado la wiki en varias secciones. Cada secci\u00f3n contiene gu\u00edas, procedimientos, est\u00e1ndares y mejores pr\u00e1cticas relevantes.</p> <ol> <li>Proyectos </li> <li>Descripci\u00f3n de los principales proyectos de Big Data en curso.  </li> <li>Roadmaps, objetivos y estados de avance.  </li> <li> <p>Metodolog\u00edas de seguimiento y reporting.</p> </li> <li> <p>Procesos y Procedimientos </p> </li> <li>Flujos de trabajo para la ingesta de datos, ETL/ELT y almacenamiento.  </li> <li>Lineamientos de aseguramiento de calidad y pruebas.  </li> <li> <p>Pol\u00edticas de seguridad y gobernanza de datos.</p> </li> <li> <p>Herramientas y Tecnolog\u00edas </p> </li> <li>Descripci\u00f3n de la stack tecnol\u00f3gica (Cloud, herramientas de BI, pipelines, etc.).  </li> <li>Gu\u00edas de instalaci\u00f3n, configuraci\u00f3n y mantenimiento.  </li> <li> <p>Recomendaciones de mejores pr\u00e1cticas.</p> </li> <li> <p>Gu\u00edas y Tutoriales </p> </li> <li>Tutoriales paso a paso para uso de plataformas internas.  </li> <li>Casos de uso y ejemplos pr\u00e1cticos.  </li> <li> <p>Documentaci\u00f3n de APIs y librer\u00edas internas.</p> </li> <li> <p>Recursos y Referencias </p> </li> <li>Documentos de referencia, libros blancos y art\u00edculos recomendados.  </li> <li>Enlaces a repositorios internos o externos.  </li> <li> <p>Est\u00e1ndares y normativas a nivel corporativo o legal.</p> </li> <li> <p>FAQ (Preguntas Frecuentes) </p> </li> <li>Respuestas a las dudas m\u00e1s comunes sobre los procesos y herramientas.  </li> <li>Consejos r\u00e1pidos para la resoluci\u00f3n de problemas habituales.</li> </ol>"},{"location":"#contribuir-a-la-wiki","title":"Contribuir a la Wiki","text":"<p>Nuestra wiki est\u00e1 en constante evoluci\u00f3n. Para mantenerla actualizada y confiable:</p> <ol> <li>Proponer Cambios: Si detectas informaci\u00f3n desactualizada o deseas agregar contenido, crea un Pull Request (o el flujo que tu organizaci\u00f3n utilice) en el repositorio correspondiente.  </li> <li>Revisi\u00f3n: Todo cambio es revisado por el equipo de QA y/o el responsable del frente correspondiente.  </li> <li>Aprobaci\u00f3n: Una vez validado, se fusiona el contenido al repositorio principal y se despliega en la wiki.</li> </ol> <p>Sugerencia: Aseg\u00farate de incluir referencias y ejemplos claros para facilitar la comprensi\u00f3n y adopci\u00f3n de tus aportes.</p>"},{"location":"#creditos-y-agradecimientos","title":"Cr\u00e9ditos y Agradecimientos","text":"<p>Agradecemos a todos los miembros del equipo que contribuyen diariamente a la construcci\u00f3n y mejora de esta wiki. Tu conocimiento y dedicaci\u00f3n hacen posible que el Equipo de Big Data siga creciendo y generando valor en la organizaci\u00f3n.</p>"},{"location":"#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<ul> <li>Revisa las Secciones Principales para ubicar la informaci\u00f3n que necesites.  </li> <li>Si eres nuevo en el equipo, consulta la Gu\u00eda de Onboarding (enlace de ejemplo) para acelerar tu proceso de adaptaci\u00f3n.  </li> <li>Participa en las discusiones y contribuye con tus conocimientos para que esta wiki sea cada vez m\u00e1s completa.</li> </ul> <p>\u00a1Bienvenido a la WikiLake y gracias por ser parte de esta gran aventura en Big Data!</p>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/","title":"Dag - Execution","text":""},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#indice","title":"\u00cdndice","text":"<ol> <li>Diagrama de Funcionamiento</li> <li>Diagrama de Registro de Nuevo DAG</li> <li>Archivo Parameters y Tablas de Ingesta - Dependencia</li> <li>Archivo Maestro de DAGs</li> </ol>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#diagrama-de-funcionamiento","title":"Diagrama de Funcionamiento","text":"<ol> <li> <p>El proceso inicia con la ejecuci\u00f3n autom\u00e1tica del dag trigger_01. Para ello, se apoya en la informaci\u00f3n de las tablas <code>config_tablas_ingesta</code>, <code>config_dependencias_dag</code> y en el archivo <code>parameters.json</code>.</p> </li> <li> <p>Una vez terminado el trigger_01 (l\u00f3gica de dependencias de dag), se ejecuta el dag trigger_02. Este dag realiza la ejecuci\u00f3n en orden, seg\u00fan las dependencias de dag y por ambiente (silver, golden y delivery).</p> </li> </ol>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#diagrama-de-registro-de-nuevo-dag","title":"Diagrama de Registro de Nuevo DAG","text":"<ol> <li> <p>Se debe registrar la dependencia entre dags en el archivo <code>parameters.json</code> en el modelo nuevo.</p> </li> <li> <p>Luego se deben registrar las tablas que se crearon, ya sea en el ambiente silver, golden y delivery, y las tablas que se usen del ambiente bronze en la tabla <code>config_tablas_ingesta</code>. En la tabla <code>config_dependencias_dag</code> se registra el nuevo dag y las tablas del ambiente anterior que se necesitan para que el dag se ejecute.</p> </li> <li> <p>Finalmente, se registra el nombre del nuevo dag en el archivo <code>slv_gld_dags_to_trigger</code>.</p> </li> </ol>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#archivo-parameters-y-tablas-de-ingesta-dependencia","title":"Archivo Parameters y Tablas de Ingesta - Dependencia","text":"<p>En este archivo JSON se registran las dependencias entre dags. Solo se registra el nombre del dag, ya sea del mismo ambiente o del ambiente anterior, en la parte de <code>dag_dependencies</code>. Se toma como ejemplo el dag de gld cliente.</p>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#ejemplo-de-registro-en-el-archivo-parameters","title":"Ejemplo de Registro en el Archivo Parameters","text":"<p>El dag del ambiente golden, para que se ejecute correctamente y con la data actualizada, depende de 3 dags del ambiente anterior que es silver:</p> <ul> <li><code>alicorp-pe-gtm-digitales-{env}-slv-modelo-cliente</code></li> <li><code>alicorp-pe-gtm-digitales-{env}-slv-modelo-producto</code></li> <li><code>alicorp-pe-gtm-digitales-{env}-slv-modelo-ventas</code></li> </ul> <p>Podr\u00eda ser que tambi\u00e9n dependa de otro dag del ambiente golden. En ese caso, se debe registrar tambi\u00e9n el nombre del dag en la parte de <code>dag_dependencies</code>.</p>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#donde-se-encuentra-el-archivo","title":"\u00bfD\u00f3nde se encuentra el Archivo?","text":"<ul> <li>El archivo se encuentra en la carpeta <code>dml</code> del modelo que se est\u00e1 desarrollando.  </li> <li>Debe ser creado tomando como referencia el archivo de otro modelo.  </li> <li>Todo se trabaja a trav\u00e9s de GitHub, donde se pueden encontrar los proyectos de Bronze, Silver y Golden y guiarse de los modelos existentes.</li> </ul>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#tabla-de-ingesta","title":"Tabla de Ingesta","text":"<p>Esta tabla contiene todos los nombres de las tablas que se usan en los dags, ya sean de bronze, silver, golden y delivery. Funciona como una tabla de monitoreo, ya que permite conocer qu\u00e9 tablas se usan en el Datalake.</p>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#funcionamiento-segun-ambiente","title":"Funcionamiento seg\u00fan Ambiente","text":"<ol> <li>Si se ejecuta un dag de silver, la tabla contiene las tablas de bronze y silver que usa el dag.</li> <li>Si se ejecuta un dag de golden, contiene tablas de silver y golden.</li> <li>Si se ejecuta un dag de delivery, contiene tablas de golden y delivery.</li> </ol>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#estructura-de-la-tabla-de-ingesta","title":"Estructura de la Tabla de Ingesta","text":"Esquema Descripci\u00f3n <ul><li>Nombre del dataset</li><li>Nombre de la tabla</li><li>Identificador de la tabla</li><li>Estado de actualizaci\u00f3n de la tabla</li><li>Estado de la tabla despu\u00e9s del proceso de actualizaci\u00f3n</li><li>Indica si la cantidad increment\u00f3</li><li>Campo usado como flag</li><li>Fecha de actualizaci\u00f3n de la tabla</li><li>Cantidad de registros</li><li>Variaci\u00f3n de cantidad</li><li>Indica si la tabla se carg\u00f3 correctamente</li><li>Plataforma a la que pertenece la tabla</li></ul> <p>nota: No es necesario completar todas las columnas si no se tiene el detalle de cada una de ellas. El insert general que se utiliza es el siguiente:</p>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#ejemplo-de-insert","title":"Ejemplo de Insert","text":"<pre><code>INSERT INTO `acpe-prod-brz.PROD_CONFIG.config_tablas_ingesta` \nVALUES('PROD__SAPS4','t001k','',true,true,false,false,CURRENT_TIMESTAMP(),0,0,true,'SAPS4');\n</code></pre>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#campos-indispensables","title":"Campos indispensables","text":"<ul> <li>Dataset</li> <li>Table_name</li> <li>Status_last_ejecution </li> <li>Si el valor es <code>true</code>, no es necesario validar la fecha de la tabla en el campo <code>last_ejecution</code>.</li> <li>Si es <code>false</code>, se debe validar que la fecha en <code>last_ejecution</code> est\u00e9 actualizada.</li> <li>Este campo es esencial para la ejecuci\u00f3n del DAG en silver; para golden y delivery puede ir en <code>true</code>.</li> <li>Last_ejecution   Indica la fecha de actualizaci\u00f3n de la tabla, con valor por defecto <code>CURRENT_TIMESTAMP()</code>.</li> <li>Plataforma</li> </ul> <p>Los otros campos puede ir con valores por defecto como se indica en el insert siguiente:</p> <pre><code>INSERT INTO `acpe-prod-brz.PROD_CONFIG.config_tablas_ingesta` \nVALUES(\u2018{dataset}\u2019,\u2019{table_name}','',true,\u2019{status_last_ejecution}',false,false,\u2019{last_ejecution}',0,0,true,'{plataforma}');\n</code></pre>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#tabla-de-dependencia","title":"Tabla de Dependencia","text":"<p>Esta tabla contiene los nombres de los dags que se ejecutan en el dag execution y cada dag contiene las tablas que se utilizan en la l\u00f3gica de los dml(.sql), estas tablas deben estar registradas en la tabla config_tablas_ingesta para que el proceso de dag execution ejecute correctamente.</p> Esquema Descripci\u00f3n <ul><li>Nombre del equipo al que pertenece el dag</li><li>Descripci\u00f3n general del dag</li><li>Descripci\u00f3n detallada del dag</li><li>Nombre del dag</li><li>Frecuencia de actualizaci\u00f3n (d\u00eda, mes, a\u00f1o)</li><li>Fecha de inicio de ejecuci\u00f3n del dag</li><li>Indica si el dag ejecut\u00f3 correctamente</li><li>Fecha fin de ejecuci\u00f3n del dag</li><li>Nombre del dataset de la tabla dependiente</li><li>Nombre de la tabla dependiente</li><li>Ambiente de la tabla dependiente</li><li>Grado de dependencia con la tabla dependiente</li><li>D\u00edas de tolerancia para actualizaci\u00f3n de la tabla dependiente</li></ul> <p>No es necesario completar todas las columnas si no se tiene el detalle de cada una de ellas. El insert general que se utiliza es el siguiente:</p> <pre><code>INSERT INTO `acpe-prod-brz.PROD_CONFIG.config_dependencias_dag` \nVALUES\n('SUPPLY','MODELO MAESTRO','tablas del modelo maestro','alicorp-pe-s4-dd-slv-modelo-maestro','DIARIA',CURRENT_TIMESTAMP(),'SUCCEEDED',CURRENT_TIMESTAMP(),'PROD__SAPS4','t001l','BRONZE','DIARIA',0);\n</code></pre>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#campos-indispensables_1","title":"Campos indispensables","text":"<ul> <li>equipo</li> <li>nombre_descriptivo_dag</li> <li>descripci\u00f3n </li> <li>recurso </li> <li>dataset</li> <li>nombre_tabla</li> <li>tipo_ingesta</li> </ul> <p>Los otros campos puede ir con valores por defecto como se indica en el insert siguiente:</p> <pre><code>INSERT INTO `acpe-prod-brz.PROD_CONFIG.config_dependencias_dag` \nVALUES(\u2018{equipo}\u2019,\u2019{nombre_descriptivo_dag}\u2019,\u2019{descripcion}\u2019,\u2019{recurso}','DIARIA',CURRENT_TIMESTAMP(),'SUCCEEDED',CURRENT_TIMESTAMP(),\u2019{dataset}\u2019,\u2019{nombre_tabla}\u2019,\u2019{tipo_ingesta}','DIARIA',0);\n</code></pre>"},{"location":"dataops/cloudComposer/DagExecution/dag-execution/#archivo-maestro-de-dags","title":"Archivo Maestro de Dags","text":"<p>En este archivo json se registra le nombre del dag para que aparezca en el proceso de ejecuci\u00f3n del dag trigger_02, de lo contrario no va a aparecer el nombre.</p> <p> </p>"},{"location":"dataops/cloudFunction/cloudFunctionParquet/","title":"Carga archivos .parquet a BigQuery","text":""},{"location":"dataops/cloudFunction/cloudFunctionParquet/#proposito-general","title":"Prop\u00f3sito General","text":"<p>Orquestar la carga autom\u00e1tica hacia BigQuery desde archivos <code>.parquet</code> depositados en un bucket de Cloud Storage.</p> <ul> <li>Schema Detection: <code>us-{env}-com-gcf-trv-schema-detection</code></li> <li>Carga BQ Unigis: <code>us-{env}-com-gcf-bq-process-load</code></li> </ul>"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#consideraciones","title":"Consideraciones","text":"<ul> <li>La extensi\u00f3n \u00fanica aceptada es <code>.parquet</code></li> <li>El nombre de la cabecera en los archivos debe considerar lo siguiente:</li> <li>Solo contener letras, n\u00fameros y guiones bajos</li> <li>Comenzar con una letra o gui\u00f3n bajo</li> <li>Tener un m\u00e1ximo de 300 caracteres</li> <li>Todo campo/columna ser\u00e1 cargado a BigQuery con tipo de dato <code>STRING</code>, a pesar de que en el archivo de carga tenga un tipo de dato diferente</li> <li>Por defecto, cada vez que se cargue un nuevo <code>.parquet</code> se a\u00f1adir\u00e1 como append a la tabla, con el nuevo contenido del archivo, pero con el mismo schema</li> </ul>"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#arquitectura-base-ejemplo-unigis-process","title":"Arquitectura Base - Ejemplo Unigis Process","text":""},{"location":"dataops/cloudFunction/cloudFunctionParquet/#recursos","title":"Recursos","text":""},{"location":"dataops/cloudFunction/cloudFunctionParquet/#desarrollo-dev","title":"Desarrollo (dev)","text":"Servicio Nombre Proyecto Descripci\u00f3n Cloud Storage <code>us_dev_stg_gst_&lt;iniciativa&gt;</code> <code>acpe-dev-brz-tmp</code> Bucket que almacena los archivos de la iniciativa asociada Cloud Functions <code>us-dev-com-gcf-trv-schema-detection</code> <code>acpe-dev-brz-tmp</code> Funci\u00f3n que detecta el schema del archivo cargado Cloud Functions <code>us-dev-com-gcf-bq-process-load</code> <code>acpe-dev-brz-tmp</code> Funci\u00f3n que se encarga de cargar la data hacia BigQuery en la capa BRONZE BigQuery - <code>acpe-dev-brz</code> Proyecto en el cual se carga la informaci\u00f3n procesada"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#calidad-qa","title":"Calidad (qa)","text":"Servicio Nombre Proyecto Descripci\u00f3n Cloud Storage <code>us_qa_stg_gst_&lt;iniciativa&gt;</code> <code>acpe-qa-brz-tmp</code> Bucket que almacena los archivos de la iniciativa asociada Cloud Functions <code>us-qa-com-gcf-trv-schema-detection</code> <code>acpe-qa-brz-tmp</code> Funci\u00f3n que detecta el schema del archivo cargado Cloud Functions <code>us-qa-com-gcf-bq-process-load</code> <code>acpe-qa-brz-tmp</code> Funci\u00f3n que se encarga de cargar la data hacia BigQuery en la capa BRONZE BigQuery - <code>acpe-qa-brz</code> Proyecto en el cual se carga la informaci\u00f3n procesada"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#produccion-prod","title":"Producci\u00f3n (prod)","text":"Servicio Nombre Proyecto Descripci\u00f3n Cloud Storage <code>us_prod_stg_gst_&lt;iniciativa&gt;</code> <code>acpe-prod-brz-tmp</code> Bucket que almacena los archivos de la iniciativa asociada Cloud Functions <code>us-prod-com-gcf-trv-schema-detection</code> <code>acpe-prod-brz-tmp</code> Funci\u00f3n que detecta el schema del archivo cargado Cloud Functions <code>us-prod-com-gcf-bq-process-load</code> <code>acpe-prod-brz-tmp</code> Funci\u00f3n que se encarga de cargar la data hacia BigQuery en la capa BRONZE BigQuery - <code>acpe-prod-brz</code> Proyecto en el cual se carga la informaci\u00f3n procesada"},{"location":"dataops/cloudFunction/cloudFunctionParquet/#codigo-fuente","title":"C\u00f3digo Fuente","text":"<ul> <li>Schema Detection</li> <li>Carga BQ Unigis</li> </ul>"},{"location":"dataops/utilidades/bash/bash-001/","title":"GitHub Workflow Bash","text":""},{"location":"dataops/utilidades/bash/bash-001/#objetivo","title":"Objetivo","text":"<p>Este script Bash automatiza la validaci\u00f3n y actualizaci\u00f3n de workflows en repositorios de GitHub pertenecientes a una organizaci\u00f3n o usuario espec\u00edfico. Su funci\u00f3n es garantizar que los archivos de workflow usen inicialmente <code>ubuntu-20.04</code> para luego actualizarlos a <code>ubuntu-24.04</code> de forma autom\u00e1tica, integrando los cambios en la rama especificada.</p>"},{"location":"dataops/utilidades/bash/bash-001/#funcionalidades","title":"Funcionalidades","text":"<ul> <li>Automatizaci\u00f3n: Clona repositorios y procesa cada uno sin intervenci\u00f3n manual.</li> <li>Validaci\u00f3n: Verifica que todos los workflows tengan configurado <code>runs-on: ubuntu-20.04</code> antes de actualizar.</li> <li>Actualizaci\u00f3n: Reemplaza la versi\u00f3n de Ubuntu en los archivos YAML y realiza commit y push a la rama especificada.</li> <li>Limpieza: Elimina los repositorios clonados una vez finalizado el proceso.</li> </ul>"},{"location":"dataops/utilidades/bash/bash-001/#codigo-del-script","title":"C\u00f3digo del Script","text":"<pre><code>#!/bin/bash  \n# Variables: configuramos nuestro usuario (o nombre de la organizaci\u00f3n) y, si es necesario, token\nUSERNAME=\"Alicorp-Digital\"\n# Opcional: exportamos el token si GitHub CLI lo requiere, por ejemplo:\n# export GITHUB_TOKEN=\"tu_token\"\n\n# Listamos todos los repositorios\nrepos=$(gh repo list $USERNAME --limit 1000 --json nameWithOwner -q '.[].nameWithOwner')\n\n\nfor repo in $repos; do\n  echo \"$repo...\"\n  git clone https://github.com/$repo.git\n  repo_dir=$(basename $repo)\n  cd \"$repo_dir\" || continue\n\n  # Verificamos que exista el directorio de workflows\n  if [ -d \".github/workflows\" ]; then\n\n    # Validamos que TODOS los workflows tengan en sus l\u00edneas 'runs-on:' solo \"ubuntu-20.04\"\n    valid=true\n    for file in .github/workflows/*.yml; do\n      if [ -f \"$file\" ]; then\n        # Extraemos las l\u00edneas que contengan \"runs-on:\"\n        runs_on_lines=$(grep \"runs-on:\" \"$file\")\n        if [ -n \"$runs_on_lines\" ]; then\n          # Si alguna de las l\u00edneas no contiene \"ubuntu-20.04\", se marca el repo como no v\u00e1lido\n          if echo \"$runs_on_lines\" | grep -q -v \"ubuntu-20.04\"; then\n             valid=false\n             echo \"Skipping $repo: $file tiene una l\u00ednea 'runs-on:' que no es ubuntu-20.04\"\n             break\n          fi\n        fi\n      fi\n    done\n\n    if [ \"$valid\" = false ]; then\n      echo \"Skipping repository $repo because not all workflows use exclusively ubuntu-20.04\"\n      cd ..\n      rm -rf \"$repo_dir\"\n      continue\n    fi\n\n    # Actualizamos cada archivo .yml dentro de la carpeta workflows\n    for file in .github/workflows/*.yml; do\n      if [ -f \"$file\" ]; then\n        echo \"Actualizando $file...\"\n        # Reemplazamos versiones antiguas por ubuntu-24.04\n        sed -i 's/runs-on: ubuntu-[0-9]\\+\\(\\.[0-9]\\+\\)\\?/runs-on: ubuntu-24.04/g' \"$file\"\n      fi\n    done\n\n    # Si hubo cambios, los comitea y empuja\n    if [ -n \"$(git status --porcelain)\" ]; then\n      git add .github/workflows\n      git commit -m \"[no ci] Actualizaci\u00f3n autom\u00e1tica: workflow a ubuntu-24.04\"\n      # Pusheamos a la rama que necesitamos\n      git push origin prod\n    fi\n  else\n    echo \"No se encontr\u00f3 el directorio .github/workflows en $repo\"\n  fi\n\n  cd ..\n  # Limpiamos el repositorio clonado\n  rm -rf \"$repo_dir\"\ndone\n\n</code></pre>"}]}